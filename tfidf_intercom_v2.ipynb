{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, StopWordsRemover}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession.builder.appName(\"intercom_tfidf\").master(\"local[8]\").getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val documents = spark.sparkContext.wholeTextFiles(\"dataWithoutBot/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 501 conversations from Intercom, used those for the PoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did some cleaning of the data, getting rid of punctuation, unnecessary spacing, taking out stopwords(they would skew the results), making everything lowercase and then tokenizing, which is just turning them into a list of words as opposed to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_spacing(inputString: String) =\n",
    "    inputString.trim.replaceAll(\" +\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline_characters(inputString: String) =\n",
    "    inputString.filter(_ >= ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "//this messes with email addresses and links, shall i set those aside first?\n",
    "\n",
    "def remove_punctuation_make_lowercase(inputString: String) =\n",
    "    inputString.replaceAll(\"\\\\p{Punct}|\\\\d\",\"\").toLowerCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(inputString: String) =\n",
    "   inputString.split(\" \").toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cleanDocuments = documents.map(_._2).map(remove_newline_characters).map(remove_multiple_spacing).map(remove_punctuation_make_lowercase).map(tokenizer).map(cleanConversation => (cleanConversation.head, cleanConversation.tail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "val stopWordsRemover = new StopWordsRemover().setInputCol(\"value\").setOutputCol(\"withoutStopWords\")\n",
    "val withoutStopWords = stopWordsRemover.transform(cleanDocuments.map(_._2).toDS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then comes the TF-IDF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF is Term Frequency. This is the number of times a word appears in a document(each conversation is a document) normalized by the number of words in the document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF is Inverse Document Frequency. This is the number of documents that contain a term normalized by the total number of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying the 2 values gives us the TF-IDF of each term. This process also turns the conversations into vectors of the same length. With a mathematical representation of the conversations, we can now start using them in mathematical ways...read ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "val countVectorizer = new CountVectorizer().setMinDF(40).setInputCol(\"withoutStopWords\").setOutputCol(\"term_frequency\")\n",
    "val countVectorizerModel = countVectorizer.fit(withoutStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vectorizedDocuments = countVectorizerModel.transform(withoutStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizedDocuments.cache()\n",
    "val idf = new IDF().setInputCol(\"term_frequency\").setOutputCol(\"tfidf\").fit(vectorizedDocuments)\n",
    "val tfidf = idf.transform(vectorizedDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val idfScoresList = idf.idf.toString.drop(1).dropRight(1).split(\",\").toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vocabularyList = countVectorizerModel.vocabulary.toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "val idfScoresLookUp = vocabularyList zip idfScoresList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ordered the terms in descending order of TF-IDF scores and printed the top 100. Remember, this score tells us what terms are deemed most important/relevant to the subject matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|        word|         idf_score|\n",
      "+------------+------------------+\n",
      "|         way|2.5050280529874214|\n",
      "|        take|2.5050280529874214|\n",
      "|      revert|2.5050280529874214|\n",
      "|     details|2.5050280529874214|\n",
      "|applications|2.5050280529874214|\n",
      "|        paid|2.5050280529874214|\n",
      "|     talking|2.5050280529874214|\n",
      "|     mondays| 2.480930501408361|\n",
      "|   thursdays| 2.480930501408361|\n",
      "|     provide| 2.480930501408361|\n",
      "|    caroline| 2.480930501408361|\n",
      "|        much| 2.457400003998167|\n",
      "|       issue| 2.457400003998167|\n",
      "|         vat| 2.434410485773468|\n",
      "|       admin| 2.434410485773468|\n",
      "|    platform| 2.434410485773468|\n",
      "|      client| 2.434410485773468|\n",
      "|        user|2.4119376299214093|\n",
      "|   integrate|2.4119376299214093|\n",
      "|         new|2.4119376299214093|\n",
      "|         pay|2.4119376299214093|\n",
      "|   shortcode|2.4119376299214093|\n",
      "|    payments|2.4119376299214093|\n",
      "|    possible|2.3684525179816704|\n",
      "|     charges|2.3684525179816704|\n",
      "|          go| 2.347399108783838|\n",
      "|      trying| 2.347399108783838|\n",
      "|     already| 2.347399108783838|\n",
      "|         per| 2.347399108783838|\n",
      "|           Â |2.3267798215811024|\n",
      "|       short|2.3267798215811024|\n",
      "|          im|2.3267798215811024|\n",
      "|    services|2.3267798215811024|\n",
      "|      moment| 2.306577114263583|\n",
      "|       takes|2.2867744869674036|\n",
      "|        days|2.2867744869674036|\n",
      "|        dont|2.2867744869674036|\n",
      "|    networks|2.2867744869674036|\n",
      "|        test|2.2867744869674036|\n",
      "|        time|2.2867744869674036|\n",
      "|         hey|2.2867744869674036|\n",
      "|         top| 2.267356401110302|\n",
      "|        work| 2.267356401110302|\n",
      "|     request| 2.267356401110302|\n",
      "|       login| 2.267356401110302|\n",
      "|        make| 2.267356401110302|\n",
      "|        look|2.2483082061396074|\n",
      "|       still| 2.229616073127455|\n",
      "|        well| 2.229616073127455|\n",
      "|       sorry| 2.229616073127455|\n",
      "|        link| 2.229616073127455|\n",
      "|       phone| 2.211266934459258|\n",
      "|        free|2.1932484289565797|\n",
      "|         set|2.1932484289565797|\n",
      "|         key|2.1932484289565797|\n",
      "|     sending| 2.175548851857179|\n",
      "|        used| 2.175548851857179|\n",
      "|     product|  2.15815710914531|\n",
      "|         try|  2.15815710914531|\n",
      "|      create|2.1242555574696285|\n",
      "|     regards|2.0914657346466377|\n",
      "|        done| 2.044212849796092|\n",
      "|        able| 2.044212849796092|\n",
      "|     delayed| 2.044212849796092|\n",
      "|       ready| 2.044212849796092|\n",
      "|    attached|2.0289453776653037|\n",
      "|         via|1.9990924145156226|\n",
      "|      shared|1.9990924145156226|\n",
      "|        also|1.9844936150944699|\n",
      "|        ussd|1.9844936150944699|\n",
      "|        want|1.9701048776423702|\n",
      "|        name|1.9559202426504139|\n",
      "|       thank|1.9559202426504139|\n",
      "|        bulk|1.9419340006756738|\n",
      "|     sandbox|1.9419340006756738|\n",
      "|       check|1.9145350264875596|\n",
      "|       share|1.9145350264875596|\n",
      "|     message|1.9145350264875596|\n",
      "|         app|1.9145350264875596|\n",
      "|        okay|1.9011120061554188|\n",
      "|     service|1.8491522672247078|\n",
      "|       error|1.8365734850178477|\n",
      "|        team|1.8241509650192904|\n",
      "|        cost|1.8241509650192904|\n",
      "|        sent|1.7997595118951313|\n",
      "|      sender|1.7997595118951313|\n",
      "|     welcome|1.7759488632014129|\n",
      "|        help|1.7642528234382213|\n",
      "|   safaricom|1.7299637499595895|\n",
      "|         kes| 1.718790449361464|\n",
      "|       using|1.7077406131748791|\n",
      "|     payment|1.6968115426426889|\n",
      "|     paybill|1.6968115426426889|\n",
      "|    response|1.6860006265384733|\n",
      "|    messages|1.6860006265384733|\n",
      "|          ok|1.6647232280911883|\n",
      "|    username| 1.654251928223893|\n",
      "|       mpesa| 1.654251928223893|\n",
      "|        back| 1.654251928223893|\n",
      "|          id|1.6438891411883465|\n",
      "+------------+------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "idfScoresLookUp.toDF(\"word\", \"idf_score\").orderBy(desc(\"idf_score\")).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is pretty encouraging at this point. I first looked to see if the various products would emerge. Spotted payments, ussd and bulk. Plus certain terms that are commonly used when talking about those products. I see paybill, mpesa -> payments. I see shared -> ussd. Also spotted sandbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something else I thought really cool is the prominence of mention of Monday and Thursday. These are th 2 days when sender IDs are raised. To be honest, even I didn't know this until I asked MC if anything outsanding happens on these days. Points to loads of customer enquiries possibly. Also tells us TF-IDF is a good place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to turn this into a more organised pipeline in preparation for the rest of the data. We have about 20,000 intercom conversation and I did this with only 500, so we should be able to get more intuitive results when we scale this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also did some reading on the what we are trying to achieve which is essentially 'Topic Modelling'.\n",
    "I'd like to try clustering the documents before getting the TF-IDF scores. I want to see what vocabulary will emerge around the different products, if the data even clusters in a product-wise manner to start with, and then compare that method with getting TF-IDF scores from the entire corpus.\n",
    "Once we are satisfied with the results of the TF-IDF, then we can train a classification model that can do real-time tagging of the conversations as they are happening and see how that goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
