{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os    \n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join('', \"conversation_data/*.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 302 conversations from Intercom, used those for the PoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had to do some cleaning, getting rid of unwanted characters and spacing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_multiple_spacing(string):\n",
    "    return re.sub(' +', ' ', string.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_newLine_character(string):\n",
    "    return string.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conversations = list()\n",
    "\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "      conversations.append(remove_multiple_spacing(remove_newLine_character(f.read())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what one conversation looks like at this point.\n",
    "The number at the start is the conversation ID.\n",
    "For now, got rid of all the context of who says what in the dialog, working with just the content of messages in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"12750452841 Hi, I have created a USSD service code in the Sandbox and I am not able to access it externally is it possible to do this? Africa's Talking typically replies in under 5m. In the meantime, these articles might help: Why am I receiving ‘Dear customer, the network is experiencing technical problems and your request was not processed. Please try again later’ response from AT API? Grace Why am I getting the error 'Supplied Authentication is Invalid'? Liz Kathure Why am I receiving 'Connection MMI code' response from AT API? Grace More in the Help Center Good afternoon When testing the service on sandbox the code will be delivered to the simulator and not the phone. You need to launch the simulator. Thanks - I keep getting this error message on the simulator A moment please for tech team. Anthony Maina Kindly look into this and revert. Hi What's your username? timothy.muchai@m-kopa.com Hi Timothy,\\xa0 Sorry for the delayed response that error you are getting is due the fact that we were unable to reach your callback. For more information on how USSD works and how to set up a call back see the docs in the link provided http://docs.africastalking.com/ussd thanks\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tokenize the conversations, which is just turning them into a list of words as opposed to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize = lambda doc: doc.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12750452841',\n",
       " 'hi,',\n",
       " 'i',\n",
       " 'have',\n",
       " 'created',\n",
       " 'a',\n",
       " 'ussd',\n",
       " 'service',\n",
       " 'code',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sandbox',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'able',\n",
       " 'to',\n",
       " 'access',\n",
       " 'it',\n",
       " 'externally',\n",
       " 'is',\n",
       " 'it',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this?',\n",
       " \"africa's\",\n",
       " 'talking',\n",
       " 'typically',\n",
       " 'replies',\n",
       " 'in',\n",
       " 'under',\n",
       " '5m.',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meantime,',\n",
       " 'these',\n",
       " 'articles',\n",
       " 'might',\n",
       " 'help:',\n",
       " 'why',\n",
       " 'am',\n",
       " 'i',\n",
       " 'receiving',\n",
       " '‘dear',\n",
       " 'customer,',\n",
       " 'the',\n",
       " 'network',\n",
       " 'is',\n",
       " 'experiencing',\n",
       " 'technical',\n",
       " 'problems',\n",
       " 'and',\n",
       " 'your',\n",
       " 'request',\n",
       " 'was',\n",
       " 'not',\n",
       " 'processed.',\n",
       " 'please',\n",
       " 'try',\n",
       " 'again',\n",
       " 'later’',\n",
       " 'response',\n",
       " 'from',\n",
       " 'at',\n",
       " 'api?',\n",
       " 'grace',\n",
       " 'why',\n",
       " 'am',\n",
       " 'i',\n",
       " 'getting',\n",
       " 'the',\n",
       " 'error',\n",
       " \"'supplied\",\n",
       " 'authentication',\n",
       " 'is',\n",
       " \"invalid'?\",\n",
       " 'liz',\n",
       " 'kathure',\n",
       " 'why',\n",
       " 'am',\n",
       " 'i',\n",
       " 'receiving',\n",
       " \"'connection\",\n",
       " 'mmi',\n",
       " \"code'\",\n",
       " 'response',\n",
       " 'from',\n",
       " 'at',\n",
       " 'api?',\n",
       " 'grace',\n",
       " 'more',\n",
       " 'in',\n",
       " 'the',\n",
       " 'help',\n",
       " 'center',\n",
       " 'good',\n",
       " 'afternoon',\n",
       " 'when',\n",
       " 'testing',\n",
       " 'the',\n",
       " 'service',\n",
       " 'on',\n",
       " 'sandbox',\n",
       " 'the',\n",
       " 'code',\n",
       " 'will',\n",
       " 'be',\n",
       " 'delivered',\n",
       " 'to',\n",
       " 'the',\n",
       " 'simulator',\n",
       " 'and',\n",
       " 'not',\n",
       " 'the',\n",
       " 'phone.',\n",
       " 'you',\n",
       " 'need',\n",
       " 'to',\n",
       " 'launch',\n",
       " 'the',\n",
       " 'simulator.',\n",
       " 'thanks',\n",
       " '-',\n",
       " 'i',\n",
       " 'keep',\n",
       " 'getting',\n",
       " 'this',\n",
       " 'error',\n",
       " 'message',\n",
       " 'on',\n",
       " 'the',\n",
       " 'simulator',\n",
       " 'a',\n",
       " 'moment',\n",
       " 'please',\n",
       " 'for',\n",
       " 'tech',\n",
       " 'team.',\n",
       " 'anthony',\n",
       " 'maina',\n",
       " 'kindly',\n",
       " 'look',\n",
       " 'into',\n",
       " 'this',\n",
       " 'and',\n",
       " 'revert.',\n",
       " 'hi',\n",
       " \"what's\",\n",
       " 'your',\n",
       " 'username?',\n",
       " 'timothy.muchai@m-kopa.com',\n",
       " 'hi',\n",
       " 'timothy,\\xa0',\n",
       " 'sorry',\n",
       " 'for',\n",
       " 'the',\n",
       " 'delayed',\n",
       " 'response',\n",
       " 'that',\n",
       " 'error',\n",
       " 'you',\n",
       " 'are',\n",
       " 'getting',\n",
       " 'is',\n",
       " 'due',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'we',\n",
       " 'were',\n",
       " 'unable',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'your',\n",
       " 'callback.',\n",
       " 'for',\n",
       " 'more',\n",
       " 'information',\n",
       " 'on',\n",
       " 'how',\n",
       " 'ussd',\n",
       " 'works',\n",
       " 'and',\n",
       " 'how',\n",
       " 'to',\n",
       " 'set',\n",
       " 'up',\n",
       " 'a',\n",
       " 'call',\n",
       " 'back',\n",
       " 'see',\n",
       " 'the',\n",
       " 'docs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'link',\n",
       " 'provided',\n",
       " 'http://docs.africastalking.com/ussd',\n",
       " 'thanks']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_conversations = [tokenize(d) for d in conversations]\n",
    "tokenized_conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then comes the TF-IDF.\n",
    "TF = Term Frequency. This is the number of times a word appears in a document(each conversation is a document) normalized by the number of words in the document.\n",
    "IDF = Inverse Document Frequency. This is the number of documents that contain a term normalized by the total number of documents.\n",
    "\n",
    "Multiplying the 2 values gives us the TF-IDF of each term.\n",
    "This process also turns the conversations into vectors of the same length. With a mathematical representation of the conversations, we can now start using them in mathematical ways...read ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize, stop_words='english')\n",
    "\n",
    "tfidf_representation = tfidf.fit_transform(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each conversation is now a vector of length of 5016. 5016 is the number of unique words in the whole corpus of documents. Each vector will contain zeros where the word in question is not in the document, and have a TF-IDF score if the word is in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5016"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_representation.toarray()[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5016"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_representation.toarray()[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5016"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_representation.toarray()[2].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I ordered the terms in descending order of TF-IDF scores and printed the top 25. Remember, this score tells us what terms are deemed important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_scores(vectorizer, tfidf_result):\n",
    "    # http://stackoverflow.com/questions/16078015/\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)[:25]\n",
    "    \n",
    "    for item in sorted_scores:\n",
    "        print(\"{0:50} Score: {1}\".format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good                                               Score: 11.033274821008682\n",
      "talking                                            Score: 10.243935691162273\n",
      "africa's                                           Score: 10.201436401483356\n",
      "way                                                Score: 9.60753187334947\n",
      "reach                                              Score: 9.603043820493477\n",
      "you:                                               Score: 9.489083138559138\n",
      "kathure                                            Score: 9.167721806064488\n",
      "liz                                                Score: 9.167721806064488\n",
      "hello                                              Score: 8.816842543125015\n",
      "hi                                                 Score: 8.646615989940456\n",
      "assist                                             Score: 8.427008284347888\n",
      "account                                            Score: 8.296337867848871\n",
      "you?                                               Score: 7.653115606614245\n",
      "help                                               Score: 7.383118597169858\n",
      "api                                                Score: 7.1653596747788155\n",
      "replies                                            Score: 6.79348664454644\n",
      "typically                                          Score: 6.79348664454644\n",
      "5m.                                                Score: 6.647611327556713\n",
      "send                                               Score: 6.575849889018653\n",
      "sender                                             Score: 6.507885453605897\n",
      "email                                              Score: 6.476257538522065\n",
      "getting                                            Score: 6.4543732554797\n",
      "sms                                                Score: 6.298235562525197\n",
      "need                                               Score: 6.2212111812577024\n",
      "thanks                                             Score: 6.2204401117253125\n"
     ]
    }
   ],
   "source": [
    "display_scores(tfidf, tfidf_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The output isn't useful at this point but in the least tells that TF-IDF is a good start. I was hoping to see the various products emerge among the top scored, instead we have:\n",
    "good, \n",
    "talking,\n",
    "africa's,\n",
    "way,\n",
    "reach.\n",
    "\n",
    "First consider that the corpus used has only 302 conversations. In total we have about 20,000, so we should get more meaningful results doing this on the whole dataset.\n",
    "Second, these particular terms all happen to be among the autoresponses the bot gives if there's no one online to handle a client request. So moving forward I'm thinking of putting those aside for starters.\n",
    "Third, I still need to work on cleaning the data a whole lot more. I see \"hi\", \"hello\", \"hey\", all variants which are really just the same thing. Along with contraction like \"I'm\" and \"I've\"...those also need to be taken care of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did some reading on the what we are trying to achieve which is essentially 'Topic Modelling'.\n",
    "\n",
    "I'd like to try clustering the documents before getting the TF-IDF scores. I want to see if the different products will emerge in those clusters given the vocabulary used around them and then compare that method with getting TF-IDF scores from the entire corpus.\n",
    "\n",
    "Once we are satisfied with the results of the TF-IDF, then we can train a classification model that can do real-time tagging of the conversations as they are happening and see how that goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
